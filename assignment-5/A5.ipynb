{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   QUESTION 2 PART B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# Function to load IDX format files (manual reading)\n",
    "def load_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read the magic number and dimensions\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        if magic == 2051:  # Magic number for images\n",
    "            n_rows, n_cols = struct.unpack(\">II\", f.read(8))\n",
    "            data = np.fromfile(f, dtype=np.uint8).reshape(size, n_rows, n_cols)\n",
    "        elif magic == 2049:  # Magic number for labels\n",
    "            data = np.fromfile(f, dtype=np.uint8)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid magic number in IDX file\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = load_idx('archive-2/train-images.idx3-ubyte')\n",
    "train_labels = load_idx('archive-2/train-labels.idx1-ubyte')\n",
    "test_images = load_idx('archive-2/t10k-images.idx3-ubyte')\n",
    "test_labels = load_idx('archive-2/t10k-labels.idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot = np.zeros((labels.shape[0], num_classes))\n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_labels = one_hot_encode(train_labels, 10)\n",
    "test_labels = one_hot_encode(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_filters = np.random.randn(32, 3, 3, 1) * 0.01  # 32 filters of size 3x3x1\n",
    "conv2_filters = np.random.randn(64, 3, 3, 32) * 0.01  # 64 filters of size 3x3x32\n",
    "fc1_weights = np.random.randn(128, 7 * 7 * 64) * 0.01  # Fully connected weights\n",
    "fc1_bias = np.zeros((128, 1))\n",
    "fc2_weights = np.random.randn(10, 128) * 0.01  # Output layer weights\n",
    "fc2_bias = np.zeros((10, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, filters):\n",
    "    n_filters, f_h, f_w, _ = filters.shape\n",
    "    n_h, n_w, _ = x.shape\n",
    "    output_h = n_h - f_h + 1\n",
    "    output_w = n_w - f_w + 1\n",
    "    output = np.zeros((output_h, output_w, n_filters))\n",
    "    for h in range(output_h):\n",
    "        for w in range(output_w):\n",
    "            for f in range(n_filters):\n",
    "                region = x[h:h+f_h, w:w+f_w, :]\n",
    "                output[h, w, f] = np.sum(region * filters[f])\n",
    "    return relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool2d(x, size=2, stride=2):\n",
    "    n_h, n_w, n_c = x.shape\n",
    "    output_h = (n_h - size) // stride + 1\n",
    "    output_w = (n_w - size) // stride + 1 \n",
    "    output = np.zeros((output_h, output_w, n_c))\n",
    "    for h in range(output_h):\n",
    "        for w in range(output_w):\n",
    "            for c in range(n_c):\n",
    "                region = x[h*stride:h*stride+size, w*stride:w*stride+size, c]\n",
    "                output[h, w, c] = np.max(region)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(image):\n",
    "\n",
    "    x = conv2d(image, conv1_filters)\n",
    "    print(f\"After conv1: {x.shape}\")\n",
    "    x = conv2d(x, conv2_filters)\n",
    "    print(f\"After conv2: {x.shape}\")\n",
    "    x = max_pool2d(x)\n",
    "    print(f\"After max pooling: {x.shape}\")\n",
    "    x = flatten(x)\n",
    "    print(f\"After flatten: {x.shape}\")\n",
    "    \n",
    "    fc_input_size = fc1_weights.shape[1]\n",
    "    assert x.size == fc_input_size, f\"Mismatch in flattened size: expected {fc_input_size}, got {x.size}\"\n",
    "    x = relu(np.dot(fc1_weights, x) + fc1_bias)\n",
    "    print(f\"After fully connected layer 1: {x.shape}\")\n",
    "    \n",
    "    x = np.dot(fc2_weights, x) + fc2_bias\n",
    "    print(f\"After fully connected layer 2: {x.shape}\")\n",
    "    \n",
    "    return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = train_images[0]\n",
    "x = conv2d(sample_image, conv1_filters)\n",
    "x = conv2d(x, conv2_filters)\n",
    "x = max_pool2d(x)\n",
    "flattened_size = x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = np.random.randn(128, flattened_size) * 0.01\n",
    "fc1_bias = np.zeros((128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = np.random.randn(128, flattened_size) * 0.01\n",
    "fc1_bias = np.zeros((128, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: (26, 26, 32)\n",
      "After conv2: (24, 24, 64)\n",
      "After max pooling: (12, 12, 64)\n",
      "After flatten: (9216,)\n",
      "After fully connected layer 1: (128, 128)\n",
      "After fully connected layer 2: (10, 128)\n",
      "Predicted probabilities: [[0.1        0.10000762 0.10000932 ... 0.10000613 0.1        0.1       ]\n",
      " [0.1        0.10000743 0.10000909 ... 0.10000598 0.1        0.1       ]\n",
      " [0.1        0.10000501 0.10000613 ... 0.10000404 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.10001907 0.10002334 ... 0.10001535 0.1        0.1       ]\n",
      " [0.1        0.10000339 0.10000415 ... 0.10000273 0.1        0.1       ]\n",
      " [0.1        0.10000314 0.10000384 ... 0.10000253 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "image = train_images[0]\n",
    "prediction = forward_pass(image)\n",
    "print(f\"Predicted probabilities: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    # Loading data from idx files\n",
    "    train_images = idx2numpy.convert_from_file('archive-2/train-images.idx3-ubyte')\n",
    "    train_labels = idx2numpy.convert_from_file('archive-2/train-labels.idx1-ubyte')\n",
    "    test_images = idx2numpy.convert_from_file('archive-2/t10k-images.idx3-ubyte')\n",
    "    test_labels = idx2numpy.convert_from_file('archive-2/t10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Subsetting and flattening data\n",
    "    subset_size = 10000\n",
    "    train_images_subset = train_images[:subset_size]\n",
    "    train_labels_subset = train_labels[:subset_size]\n",
    "    train_images_flattened = train_images_subset.reshape(subset_size, -1)\n",
    "\n",
    "    # Splitting data into train and test sets\n",
    "    split_index = int(0.8 * subset_size)\n",
    "    train_images_train = train_images_flattened[:split_index]\n",
    "    train_labels_train = train_labels_subset[:split_index]\n",
    "    train_images_test = train_images_flattened[split_index:]\n",
    "    train_labels_test = train_labels_subset[split_index:]\n",
    "\n",
    "    return train_images_train, train_labels_train, train_images_test, train_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        # Convert labels to +1 and -1 for binary classification\n",
    "        y = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for linear SVM without scaling: 11.33%\n",
      "Testing accuracy for linear SVM without scaling: 11.05%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Main execution\n",
    "train_images_train, train_labels_train, train_images_test, train_labels_test = load_and_preprocess_data()\n",
    "\n",
    "# Train Linear SVM\n",
    "svm_clf_linear = LinearSVM(learning_rate=0.001, lambda_param=0.01, epochs=1000)\n",
    "svm_clf_linear.fit(train_images_train, train_labels_train)\n",
    "\n",
    "# Predictions and accuracy\n",
    "train_predictions_linear = svm_clf_linear.predict(train_images_train)\n",
    "train_accuracy_linear = accuracy(train_labels_train, train_predictions_linear)\n",
    "print(f\"Training accuracy for linear SVM without scaling: {train_accuracy_linear * 100:.2f}%\")\n",
    "\n",
    "test_predictions_linear = svm_clf_linear.predict(train_images_test)\n",
    "test_accuracy_linear = accuracy(train_labels_test, test_predictions_linear)\n",
    "print(f\"Testing accuracy for linear SVM without scaling: {test_accuracy_linear * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
